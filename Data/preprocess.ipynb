{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oyjiNnxOq8FW"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import sqlite3\n",
    "import string\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_train = sqlite3.connect(\"./raw/data_train.db\")\n",
    "crs_train = conn_train.cursor()\n",
    "data_train = pd.read_sql_query(\"SELECT * FROM data ORDER BY RANDOM()\", conn_train)\n",
    "\n",
    "conn_test = sqlite3.connect(\"./raw/data_test.db\")\n",
    "crs_test = conn_test.cursor()\n",
    "data_test = pd.read_sql_query(\"SELECT * FROM data ORDER BY RANDOM()\", conn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "yRh_F7Q0rPEG"
   },
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame()\n",
    "train_data['title'] = data_train.Posts\n",
    "train_data['label'] = data_train.Emotion\n",
    "\n",
    "test_data = pd.DataFrame()\n",
    "test_data['title'] = data_test.Posts\n",
    "test_data['label'] = data_test.Emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "mymjJ5ucsTts"
   },
   "outputs": [],
   "source": [
    "from vncorenlp import VnCoreNLP\n",
    "rdrsegmenter = VnCoreNLP(\"./vncorenlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWord():\n",
    "   f = open(\"../StopWords/vietnamese-stopwords-dash.txt\", \"r\")\n",
    "   stopWords = f.readlines()\n",
    "   for idx,line  in enumerate(stopWords):\n",
    "      stopWords[idx] = line.replace(\"\\n\", \"\").strip()\n",
    "      if (len(stopWords[idx]) <= 0):\n",
    "         stopWords.pop(idx)\n",
    "   return stopWords\n",
    "\n",
    "stopWords = removeStopWord()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict size:  73901\n",
      "['a', 'A', 'a-ba-giua', 'a-ba-toa', 'a_bàng']\n"
     ]
    }
   ],
   "source": [
    "def removeOutOfDict():\n",
    "   f = open(\"../Viet74K.txt\", \"r\")\n",
    "   dictWords = f.readlines()\n",
    "   for idx,line  in enumerate(dictWords):\n",
    "      dictWords[idx] = line.replace(\"\\n\", \"\").strip()\n",
    "      dictWords[idx] = dictWords[idx].replace(\" \", \"_\")\n",
    "      if (len(dictWords[idx]) <= 0):\n",
    "         dictWords.pop(idx)\n",
    "   return dictWords\n",
    "\n",
    "dictWords = removeOutOfDict()\n",
    "print(\"Dict size: \", len(dictWords))\n",
    "print(dictWords[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Emoticon_Dict.p', 'rb') as fp:\n",
    "    Emoticon_Dict = pickle.load(fp)\n",
    "\n",
    "def remove_emoticons(text):\n",
    "    emoticon_pattern = re.compile(\n",
    "        u'(' + u'|'.join(k for k in Emoticon_Dict) + u')')\n",
    "    return emoticon_pattern.sub(r'', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../teencode.txt', 'rb') as f:\n",
    "    teencode = f.read().decode('utf-8').split('\\n')\n",
    "    list_teencode = []\n",
    "    for i in teencode:\n",
    "        list_teencode.append(i.split('\\t'))\n",
    "\n",
    "def change_teencode(text):\n",
    "    text_list = text.split(\" \")\n",
    "    for i in range(len(text_list)):\n",
    "        for j in list_teencode:\n",
    "            if text_list[i] == j[0]:\n",
    "                text_list[i] = j[1]\n",
    "    return \" \".join(text_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^`{|}~“”’—–…̣̀́̃̉\n"
     ]
    }
   ],
   "source": [
    "puncs = string.punctuation + '“”’—–… ̣ ̀ ́ ̃ ̉'\n",
    "puncs = puncs.replace(\"_\", \"\")\n",
    "puncs = puncs.replace(\" \", \"\")\n",
    "print(puncs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "nr6JR50jrk_1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tao xong cầm bút tay_phải', 'đi_ngoài đường trái sống giỏi đấy', 'ma tơi ném cốc mặt đừng trách', 'ảnh tỉ đăng đăng cặc đéo nhàm bất_hạnh vãi', 'kết có_hậu đầu khấc vợ tương_lai kia kìa', 'xàm lồn giáo_dục đéo', 'bệnh bé quyển sổ bảo mày viết xong cầm bút run_run bảo tao viết dòng chả vẽ bậy vào_sổ vứt tiu tờ giấy xé kỉ_niệm dọn giấy_tờ lỗi ông_bà trân_trọng chia xuân nhi', 'thế_giới tồn_tại hai chữ công_bằng sai ta thông_thường vạn phạm sai_lầm hít thở soi_xét để_tâm kẻ sống ngày_mai tươi_đẹp hổ_thẹn hôm_nay', 'đéo mẹ bệnh_viện chỗ gửi xe niêm_yết rõ_ràng kêu_ca', 'tiếc đẹp_trai mờ chim']\n"
     ]
    }
   ],
   "source": [
    "train_text = train_data['title'].to_list()\n",
    "train_labels = train_data['label'].to_list()\n",
    "train_text = [re.sub(r'^\\s+|\\s+$', '', str(t)) for t in train_text]\n",
    "train_text = [str(t).strip() for t in train_text]\n",
    "train_text = [re.sub(r'\\s\\s+', ' ', str(t)) for t in train_text]\n",
    "train_text = [re.sub(r'\\n+', ' ', str(t)) for t in train_text]\n",
    "train_text = [re.sub(r'\\d+', '', str(t)) for t in train_text]\n",
    "train_text = [re.sub(r'http\\S+', '', str(t)) for t in train_text]\n",
    "train_text = [re.sub(r'\\.+', '', str(t)) for t in train_text]\n",
    "train_text = [remove_emoji(str(t)) for t in train_text]\n",
    "train_text = [remove_emoticons(str(t)) for t in train_text]\n",
    "train_text = [re.sub(r'[{}]'.format(puncs), '', str(t)) for t in train_text]\n",
    "train_text = [str(t).replace(\"_\", \" \") for t in train_text]\n",
    "train_text = [str(t).strip() for t in train_text]\n",
    "train_text = [str(t).lower() for t in train_text]\n",
    "train_text = [str(t).replace(\"cre\", \"\") for t in train_text]\n",
    "train_text = [str(t).replace(\"post hộ mem\", \"\") for t in train_text]\n",
    "train_text = [str(t).replace(\"posthộ\", \"\") for t in train_text]\n",
    "train_text = [str(t).replace(\"post hộ\", \"\") for t in train_text]\n",
    "train_text = [change_teencode(str(t)) for t in train_text]\n",
    "train_text = [rdrsegmenter.tokenize(t) for t in train_text]\n",
    "for idx, text in enumerate(train_text):\n",
    "    tmp = []\n",
    "    for idx2, text2 in enumerate(text):\n",
    "        tmp2 = []\n",
    "        for idx3, text3 in enumerate(text2):\n",
    "            if text3 not in puncs:\n",
    "                tmp2.append(\" \"+text3)\n",
    "        tmp.append(\"\".join(tmp2).strip())\n",
    "    train_text[idx] = \" \".join(tmp).strip()\n",
    "train_text = [re.sub(r'\\s+_\\s+', '_', str(t)) for t in train_text]\n",
    "train_text = [\" \".join([word for word in text.split(\n",
    "    \" \") if word not in stopWords]) for text in train_text]\n",
    "train_text = [\" \".join([word for word in text.split(\n",
    "    \" \") if word in dictWords]) for text in train_text]\n",
    "train_text = [str(t).strip() for t in train_text]\n",
    "for idx, text in enumerate(train_text):\n",
    "    if len(text) <= 0:\n",
    "        train_text.pop(idx)\n",
    "        train_labels.pop(idx)\n",
    "print(train_text[:10])\n",
    "\n",
    "test_text = test_data['title'].to_list()\n",
    "test_labels = test_data['label'].to_list()\n",
    "test_text = [re.sub(r'^\\s+|\\s+$', '', str(t)) for t in test_text]\n",
    "test_text = [str(t).strip() for t in test_text]\n",
    "test_text = [re.sub(r'\\s\\s+', ' ', str(t)) for t in test_text]\n",
    "test_text = [re.sub(r'\\n+', ' ', str(t)) for t in test_text]\n",
    "test_text = [re.sub(r'\\d+', '', str(t)) for t in test_text]\n",
    "test_text = [re.sub(r'http\\S+', '', str(t)) for t in test_text]\n",
    "test_text = [re.sub(r'\\.+', '', str(t)) for t in test_text]\n",
    "test_text = [remove_emoji(str(t)) for t in test_text]\n",
    "test_text = [remove_emoticons(str(t)) for t in test_text]\n",
    "test_text = [re.sub(r'[{}]'.format(puncs), '', str(t)) for t in test_text]\n",
    "test_text = [str(t).replace(\"_\", \" \") for t in test_text]\n",
    "test_text = [str(t).strip() for t in test_text]\n",
    "test_text = [str(t).lower() for t in test_text]\n",
    "test_text = [str(t).replace(\"cre\", \"\") for t in test_text]\n",
    "test_text = [str(t).replace(\"post hộ mem\", \"\") for t in test_text]\n",
    "test_text = [str(t).replace(\"posthộ\", \"\") for t in test_text]\n",
    "test_text = [str(t).replace(\"post hộ\", \"\") for t in test_text]\n",
    "test_text = [change_teencode(str(t)) for t in test_text]\n",
    "test_text = [rdrsegmenter.tokenize(t) for t in test_text]\n",
    "for idx, text in enumerate(test_text):\n",
    "    tmp = []\n",
    "    for idx2, text2 in enumerate(text):\n",
    "        tmp2 = []\n",
    "        for idx3, text3 in enumerate(text2):\n",
    "            if text3 not in puncs:\n",
    "                tmp2.append(\" \"+text3)\n",
    "        tmp.append(\"\".join(tmp2).strip())\n",
    "    test_text[idx] = \" \".join(tmp).strip()\n",
    "test_text = [re.sub(r'\\s+_\\s+', '_', str(t)) for t in test_text]\n",
    "test_text = [\" \".join([word for word in text.split(\n",
    "    \" \") if word not in stopWords]) for text in test_text]\n",
    "test_text = [\" \".join([word for word in text.split(\n",
    "    \" \") if word in dictWords]) for text in test_text]\n",
    "test_text = [str(t).strip() for t in test_text]\n",
    "for idx, text in enumerate(test_text):\n",
    "    if len(text) <= 0:\n",
    "        test_text.pop(idx)\n",
    "        test_labels.pop(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_train_dirty = sqlite3.connect(\"./raw/data_train_dirty.db\")\n",
    "crs_train_dirty = conn_train_dirty.cursor()\n",
    "\n",
    "conn_test_dirty = sqlite3.connect(\"./raw/data_test_dirty.db\")\n",
    "crs_test_dirty = conn_test_dirty.cursor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "537"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_dirty = pd.DataFrame()\n",
    "data_test_dirty = pd.DataFrame()\n",
    "data_train_dirty['Posts'] = train_text\n",
    "data_train_dirty['Emotion'] = train_labels\n",
    "data_test_dirty['Posts'] = test_text\n",
    "data_test_dirty['Emotion'] = test_labels\n",
    "data_train_dirty.to_sql('data', conn_train_dirty,\n",
    "                        if_exists='replace', index=False)\n",
    "data_test_dirty.to_sql('data', conn_test_dirty,\n",
    "                       if_exists='replace', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_train_clean = sqlite3.connect(\"./raw/data_train_clean.db\")\n",
    "crs_train_clean = conn_train_clean.cursor()\n",
    "\n",
    "conn_test_clean = sqlite3.connect(\"./raw/data_test_clean.db\")\n",
    "crs_test_clean = conn_test_clean.cursor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "533"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_clean = pd.DataFrame()\n",
    "data_test_clean = pd.DataFrame()\n",
    "data_train_clean['Posts'] = train_text\n",
    "data_train_clean['Emotion'] = train_labels\n",
    "data_test_clean['Posts'] = test_text\n",
    "data_test_clean['Emotion'] = test_labels\n",
    "data_train_clean.to_sql('data', conn_train_clean,\n",
    "                        if_exists='replace', index=False)\n",
    "data_test_clean.to_sql('data', conn_test_clean,\n",
    "                       if_exists='replace', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "36b5d234dcadb63a5dcffe129e6467f293a83c75bbea9a5662c39cecefcc4dbb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
